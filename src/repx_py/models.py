import json
import logging
import sys
from abc import ABC, abstractmethod
from pathlib import Path
from typing import (
    Any,
    Callable,
    Dict,
    Iterable,
    Iterator,
    List,
    Optional,
    Sequence,
    Set,
    Tuple,
    Union,
    overload,
)

import pandas as pd

logger = logging.getLogger(__name__)


class ArtifactResolver(ABC):
    """
    Abstract base class for strategies that locate physical output files
    for a given job.
    """

    @abstractmethod
    def resolve_path(self, job: "JobView", relative_path: str) -> Path:
        """
        Resolves a path relative to the job's output directory.
        Example: resolve_path(job, "results.csv")
        """
        pass


class LocalCacheResolver(ArtifactResolver):
    """
    Resolves artifacts located in a local .repx-cache directory (or store__base),
    typically generated by the debug-runner or the production runner.
    """

    def __init__(self, cache_dir: Union[str, Path] = ".repx-cache"):
        self.cache_dir = Path(cache_dir).resolve()

    def resolve_path(self, job: "JobView", relative_path: str) -> Path:
        # Standard layout: <base_dir>/<job_id>/out/<filename>
        return self.cache_dir / job.id / "out" / relative_path


class ManifestResolver(ArtifactResolver):
    """
    Resolves artifacts based on a pre-computed dictionary mapping job IDs to
    their output directories.
    """

    def __init__(self, job_output_map: Dict[str, Union[str, Path]]):
        self.mapping = {k: Path(v) for k, v in job_output_map.items()}

    def resolve_path(self, job: "JobView", relative_path: str) -> Path:
        if job.id not in self.mapping:
            raise FileNotFoundError(
                f"No output path recorded for job '{job.id}' in the provided manifest."
            )
        return self.mapping[job.id] / relative_path


class JobView:
    """
    A read-only view of a single job's metadata, effective parameters, and capabilities
    to locate its physical output artifacts using a Resolver.
    """

    def __init__(self, job_id: str, experiment: "Experiment"):
        self._id = job_id
        self._exp = experiment
        self._data = self._exp._get_complete_job_data(self._id)

    @property
    def id(self) -> str:
        return self._id

    @property
    def name(self) -> str:
        return self._data.get("name", self._id)

    @property
    def stage_type(self) -> str:
        return self._data.get("stage_type", "simple")

    @property
    def executable_path(self) -> str | None:
        if self.stage_type == "simple":
            return self._data.get("executables", {}).get("main", {}).get("path")
        return None

    @property
    def input_mappings(self) -> List[Dict[str, Any]]:
        if self.stage_type == "simple":
            return self._data.get("executables", {}).get("main", {}).get("inputs", [])
        elif self.stage_type == "scatter-gather":
            return (
                self._data.get("executables", {}).get("scatter", {}).get("inputs", [])
            )
        return []

    @property
    def outputs(self) -> Dict[str, str]:
        if self.stage_type == "simple":
            return self._data.get("executables", {}).get("main", {}).get("outputs", {})
        elif self.stage_type == "scatter-gather":
            return (
                self._data.get("executables", {}).get("gather", {}).get("outputs", {})
            )
        return {}

    @property
    def params(self) -> Dict[str, Any]:
        return self._data.get("params", {})

    @property
    def effective_params(self) -> Dict[str, Any]:
        return self._data.get("effective_params", {})

    @property
    def dependencies(self) -> "JobCollection":
        parent_ids = set()
        for mapping in self.input_mappings:
            if pid := mapping.get("job_id"):
                if pid != "self":
                    parent_ids.add(pid)
        return JobCollection(self._exp, parent_ids)

    def get_output_path(self, output_key: str) -> Path:
        template = self.outputs.get(output_key)
        if not template:
            raise KeyError(
                f"Output key '{output_key}' not found in job '{self.id}'. "
                f"Available keys: {list(self.outputs.keys())}"
            )

        if template.startswith("$out/"):
            relative_path = template[len("$out/") :]
        else:
            relative_path = template

        return self._exp.resolver.resolve_path(self, relative_path)

    def load_csv(self, output_key_or_filename: str, **kwargs) -> pd.DataFrame:
        if output_key_or_filename in self.outputs:
            path = self.get_output_path(output_key_or_filename)
        else:
            path = self._exp.resolver.resolve_path(self, output_key_or_filename)

        logger.debug(f"Loading CSV for job {self.id} from {path}")
        return pd.read_csv(path, **kwargs)

    def __getattr__(self, key: str) -> Any:
        if key in self._data:
            return self._data[key]
        raise AttributeError(f"'JobView' object has no attribute or data key '{key}'")

    def __repr__(self) -> str:
        return f"<JobView id={self.id} name={self.name}>"


class JobCollection(Sequence[JobView]):
    def __init__(self, experiment: "Experiment", job_ids: Iterable[str]):
        self._exp = experiment
        self._job_ids = list(job_ids)

    def filter(
        self, predicate: Optional[Callable[[JobView], bool]] = None, **kwargs
    ) -> "JobCollection":
        def match(job: JobView) -> bool:
            if predicate and not predicate(job):
                return False

            for key, required_value in kwargs.items():
                if "__" in key:
                    attr, op = key.split("__", 1)
                    if not hasattr(job, attr):
                        return False
                    actual_value = getattr(job, attr)

                    if op == "startswith":
                        if not str(actual_value).startswith(required_value):
                            return False
                    elif op == "endswith":
                        if not str(actual_value).endswith(required_value):
                            return False
                    elif op == "contains":
                        if required_value not in str(actual_value):
                            return False
                    else:
                        logger.warning(f"Unknown filter operator: __{op}")
                        if actual_value != required_value:
                            return False
                else:
                    if not hasattr(job, key) or getattr(job, key) != required_value:
                        return False

            return True

        filtered_ids = [
            job_id
            for job_id in self._job_ids
            if (job := self._exp.get_job(job_id)) and match(job)
        ]
        return JobCollection(self._exp, filtered_ids)

    def to_dataframe(self) -> pd.DataFrame:
        data = []
        for job_id in self._job_ids:
            job = self._exp.get_job(job_id)
            row = {"job_id": job.id, "name": job.name}
            row.update(job.effective_params)
            data.append(row)

        if not data:
            return pd.DataFrame()

        df = pd.json_normalize(data)
        if "job_id" in df.columns and df["job_id"].is_unique:
            df = df.set_index("job_id")

        return df

    def __iter__(self) -> Iterator[JobView]:
        for job_id in self._job_ids:
            yield self._exp.get_job(job_id)

    def __len__(self) -> int:
        return len(self._job_ids)

    @overload
    def __getitem__(self, index: int) -> JobView: ...

    @overload
    def __getitem__(self, index: slice) -> "JobCollection": ...

    def __getitem__(self, index: Union[int, slice]) -> Union[JobView, "JobCollection"]:
        if isinstance(index, slice):
            return JobCollection(self._exp, self._job_ids[index])
        return self._exp.get_job(self._job_ids[index])

    def __repr__(self) -> str:
        return f"<JobCollection size={len(self)}>"


class Experiment:
    """The main entry point for interacting with a RepX lab."""

    def __init__(
        self,
        lab_path: Union[str, Path, None] = None,
        resolver: Optional[ArtifactResolver] = None,
        _preloaded_metadata: Optional[Dict[str, Any]] = None,
    ):
        self.resolver = resolver or LocalCacheResolver()

        if _preloaded_metadata:
            self.path = Path(".")
            self._metadata = _preloaded_metadata
        elif lab_path:
            self.path = Path(lab_path).resolve()
            self._load_lab_manifest()
        else:
            raise ValueError(
                "Either 'lab_path' or '_preloaded_metadata' must be provided."
            )

        self._job_view_cache: Dict[str, JobView] = {}
        self._effective_params_cache = self._calculate_all_effective_params()

        self._job_to_run_map: Dict[str, str] = {}
        for run_name, run_data in self.runs().items():
            for job_id in run_data.get("jobs", []):
                self._job_to_run_map[job_id] = run_name

    @classmethod
    def from_run_metadata(
        cls, metadata_path: Union[str, Path], store_base: Union[str, Path]
    ) -> "Experiment":
        """
        Factory method to create an Experiment context from a specific run's metadata file.
        Useful inside an Analysis stage where only specific upstream metadata
        and a base store path are available.

        Args:
            metadata_path: Path to the 'metadata__*.json' file provided by inputs.
            store_base: The root path where job outputs are stored (usually 'store__base').
        """
        meta_path = Path(metadata_path)
        if not meta_path.exists():
            raise FileNotFoundError(f"Metadata file not found: {meta_path}")

        with open(meta_path, "r") as f:
            run_data = json.load(f)

        if run_data.get("type") != "run":
            raise ValueError(
                f"Expected metadata type 'run', got '{run_data.get('type')}'"
            )

        run_name = run_data.get("name", "unknown_run")

        synthetic_metadata = {
            "root": {"generated": True},
            "runs": {run_name: run_data},
            "jobs": run_data.get("jobs", {}),
        }

        store_path = Path(store_base)
        if (store_path / "outputs").is_dir():
            resolver_path = store_path / "outputs"
        else:
            resolver_path = store_path

        return cls(
            lab_path=None,
            resolver=LocalCacheResolver(cache_dir=resolver_path),
            _preloaded_metadata=synthetic_metadata,
        )

    def _load_lab_manifest(self):
        """Internal method to discover and load metadata from a Lab directory."""
        lab_dir = self.path / "lab"
        if not lab_dir.exists():
            lab_dir = self.path

        manifest_candidates = list(lab_dir.glob("*.json"))
        if not manifest_candidates:
            if (self.path / "lab").exists():
                manifest_candidates = list((self.path / "lab").glob("*.json"))

        if not manifest_candidates:
            raise FileNotFoundError(
                f"Could not find a lab manifest JSON file in {lab_dir}"
            )

        manifest_path = manifest_candidates[0]
        with open(manifest_path, "r") as f:
            lab_manifest = json.load(f)

        top_metadata_rel_path = lab_manifest.get("metadata")
        if not top_metadata_rel_path:
            raise ValueError(
                f"Lab manifest at {manifest_path} is missing 'metadata' field."
            )

        root_metadata_path = self.path / top_metadata_rel_path
        if not root_metadata_path.is_file():
            raise FileNotFoundError(f"Root metadata not found at {root_metadata_path}")

        with open(root_metadata_path, "r") as f:
            root_metadata = json.load(f)

        all_runs_data = {}
        all_jobs_data = {}

        for run_rel_path in root_metadata.get("runs", []):
            run_metadata_path = self.path / run_rel_path
            if not run_metadata_path.is_file():
                logger.warning(
                    f"Could not find metadata for run at {run_metadata_path}"
                )
                continue

            with open(run_metadata_path, "r") as f:
                run_data = json.load(f)

            run_name = run_data.get("name")
            if not run_name:
                logger.warning(
                    f"Run metadata at {run_metadata_path} is missing 'name' field."
                )
                continue

            all_runs_data[run_name] = run_data
            all_jobs_data.update(run_data.get("jobs", {}))

        self._metadata = {
            "root": root_metadata,
            "runs": all_runs_data,
            "jobs": all_jobs_data,
        }

    def _get_single_effective_params(
        self, job_id: str, all_jobs_data: Dict, visiting: Set[str], memo: Dict
    ) -> Dict[str, Any]:
        if job_id in memo:
            return memo[job_id]
        if job_id in visiting:
            raise RecursionError(f"Circular dependency detected at: {job_id}")

        visiting.add(job_id)
        job_data = all_jobs_data.get(job_id)
        if not job_data:
            raise KeyError(f"Job ID '{job_id}' not found in metadata.")

        temp_view = JobView(job_id, self)
        input_mappings = temp_view.input_mappings

        effective_params: Dict[str, Any] = {}
        for dep_mapping in input_mappings:
            if dep_id := dep_mapping.get("job_id"):
                if dep_id == "self":
                    continue
                if dep_id in all_jobs_data:
                    effective_params.update(
                        self._get_single_effective_params(
                            dep_id, all_jobs_data, visiting, memo
                        )
                    )

        effective_params.update(job_data.get("params", {}))
        visiting.remove(job_id)
        memo[job_id] = effective_params
        return effective_params

    def _calculate_all_effective_params(self) -> Dict[str, Dict]:
        all_jobs_data = self._metadata.get("jobs", {})
        if not all_jobs_data:
            return {}

        memo: Dict[str, Dict] = {}
        for job_id in all_jobs_data:
            if job_id not in memo:
                try:
                    self._get_single_effective_params(
                        job_id, all_jobs_data, set(), memo
                    )
                except KeyError:
                    memo[job_id] = all_jobs_data.get(job_id, {}).get("params", {})

        return memo

    @property
    def effective_params(self) -> Dict[str, Dict]:
        return self._effective_params_cache

    def _get_complete_job_data(self, job_id: str) -> Dict[str, Any]:
        raw_data = self._metadata.get("jobs", {}).get(job_id, {}).copy()
        if hasattr(self, "_effective_params_cache"):
            effective_params = self._effective_params_cache.get(job_id, {})
            raw_data["effective_params"] = effective_params
        return raw_data

    def get_job(self, job_id: str) -> JobView:
        if job_id not in self._job_view_cache:
            if job_id not in self._metadata.get("jobs", {}):
                raise KeyError(f"Job ID '{job_id}' not found.")
            self._job_view_cache[job_id] = JobView(job_id, self)
        return self._job_view_cache[job_id]

    def get_run_for_job(self, job_id: str) -> Tuple[str, Dict[str, Any]]:
        run_name = self._job_to_run_map.get(job_id)
        if not run_name:
            raise KeyError(f"Could not find a run containing job '{job_id}'.")
        return run_name, self.runs()[run_name]

    def jobs(self) -> JobCollection:
        return JobCollection(self, self._metadata.get("jobs", {}).keys())

    def runs(self) -> Dict[str, Any]:
        return self._metadata.get("runs", {})
